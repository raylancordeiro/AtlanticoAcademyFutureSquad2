{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122c10ed",
   "metadata": {},
   "source": [
    "# Atlântico Academy Future\n",
    "### VISÃO GERAL\n",
    "Uma empresa contratante deseja estabelecer termos de maior relevância em um documento específico. Neste caso, considere o histórico de exames, consultas e procedimentos realizados por um paciente. Um sistema deve ser desenvolvido para que o médico possa ter uma visão geral do histórico do paciente sem a necessidade de analisar documento por documento. Com\n",
    "base nesta importância, vamos desenvolver uma etapa deste sistema. Tokenizar um texto,realizar remoção de stopwords, aplicar o processo de lematização e fazer uma análise quantitativa deste. Neste caso, vamos comparar duas estratégias , se possível. A primeira utilizando a lib stanza e a segunda uma análise com base em acesso a um dicionário."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab2a4d",
   "metadata": {},
   "source": [
    "# 1) Carregar o conjunto de documentos em PDF e armazená-los em alguma estrutura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911d1ec7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_pdf_file(file):\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    page_content_all= ''\n",
    "    for i in range(number_of_pages):\n",
    "        page_content = read_pdf.getPage(i).extractText()\n",
    "        page_content_all += page_content\n",
    "    return page_content_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8706664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pfds(patch_files):\n",
    "    os.chdir(patch_files)\n",
    "    file_base_all = []\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_base_all.append(read_pdf_file(os.path.join(patch_files, file)))\n",
    "    return file_base_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98de2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_and_remove_breaks(base):\n",
    "    list_parsed = []\n",
    "    for i in range(len(base)):\n",
    "        list_parsed.append(re.sub('\\n', '', base[i]))\n",
    "    return list_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad73f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = join_and_remove_breaks(load_pfds(r'C:\\Users\\r211315\\Documents\\ia_express\\arquivos'))\n",
    "corpus = join_and_remove_breaks(load_pfds(r'/home/vagnersv/my_tensorflow/arquivos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7895825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "693a1819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uma TARTARUGA lenta pula sobre a raposa preguiçosa. QUE RAPASA DO BARALHO! ',\n",
       " 'Uma rápida RAPOSA marrom pula sobre o CÃO preguiçoso. Que raposa marota! ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe1ee7",
   "metadata": {},
   "source": [
    "# 2) Realizar o pré-processamento destes ( tokenização e remoção de stop words, deixar todos os caracteres minúsculos...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a1000",
   "metadata": {},
   "source": [
    "Remoção de Palavras Vazias(e.g., artigos, preposições, etc.), que possuem alta frequência em todos os documentos, podem ser removidas da contagem para melhorar a distinção entre documentos\n",
    "\n",
    "[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77ccf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i]= corpus[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e84ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vagnersv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "def removestopwords(texto):\n",
    "    frases = []\n",
    "    for palavras in texto:\n",
    "        semstop = [p for p in palavras.split() if p not in stopwords]\n",
    "        frases.append(semstop)\n",
    "    return frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81eaa492",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sem_stopwords = removestopwords(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47bd9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(corpus, seperator=' '):\n",
    "    return  seperator.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afe83b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sem_stopwords= list(map(convert_list_to_string, corpus_sem_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sem_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd1e58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "#######\n",
    "# https://stackoverflow.com/questions/26126442/combining-text-stemming-and-removal-of-punctuation-in-nltk-and-scikit-learn\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(texto):\n",
    "    tokens = nltk.word_tokenize(texto)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542f9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sem_stopwors_tokenizada= list(map(tokenize, corpus_sem_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e7d0c",
   "metadata": {},
   "source": [
    "# 3) Lematização com a Lib stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stanza_portugues():\n",
    "    \"\"\"\n",
    "    Faz o download do stanza em portugues\n",
    "    \"\"\"\n",
    "    stanza.download(lang='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só é necessário executar o download uma vez\n",
    "#download_stanza_portugues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_and_lemmatizer(text):\n",
    "    \"\"\"\n",
    "        Performs tokenization and lemmatization on input text\n",
    "\n",
    "    Args:\n",
    "        text: A string with the content of the text\n",
    "\n",
    "    Returns:\n",
    "        A stanza Document with the tokens and lemmas\n",
    "\n",
    "    \"\"\"\n",
    "    nlp = stanza.Pipeline('pt', processors='tokenize,mwt,pos,lemma')\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123613ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nlp_doc(doc):\n",
    "    \"\"\"\n",
    "    Imprime os tokens (somente para debug)\n",
    "    \"\"\"\n",
    "    sentence_id = 0\n",
    "    for sentence in doc.sentences:\n",
    "        sentence_id += 1\n",
    "        print('\\nSentença {}:'.format(sentence_id))\n",
    "        for word in sentence.words:\n",
    "            print('palavra = {}, lema = {}, id = {}'.format(word.text, word.lemma, word.id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31956673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory_path = r'C:\\Users\\r211315\\Documents\\ia_express\\arquivos'\n",
    "#directory_path = r'/home/vagnersv/my_tensorflow/arquivos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "txt3_processed = tokenizer_and_lemmatizer(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_nlp_doc(txt3_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08846145",
   "metadata": {},
   "source": [
    "# 4) Lematização manual com inspiração no trabalho descrito no [link](https://github.com/rikarudo/LemPORT) (Atividade desafio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6cafbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a6ff37f",
   "metadata": {},
   "source": [
    "# 5) Implementar API para determinar as seguintes informações do resultados obtidos em 3 e/ou 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4a411",
   "metadata": {},
   "source": [
    "# 5.1) Term Frequency (TF):\n",
    "𝑇𝐹 = 𝑞𝑡𝑑 𝑑𝑒 𝑜𝑐𝑜𝑟𝑟ê𝑛𝑐𝑖𝑎 𝑑𝑜 𝑡𝑒𝑟𝑚𝑜 𝑒𝑚 𝑢𝑚 𝑡𝑒𝑥𝑡𝑜 / 𝑞𝑢𝑎𝑛𝑡𝑖𝑑𝑎𝑑𝑒 𝑡𝑜𝑡𝑎𝑙 𝑑𝑒 𝑝𝑎𝑙𝑎𝑣𝑟𝑎𝑠 𝑑𝑜 𝑡𝑒𝑥𝑡𝑜\n",
    "\n",
    "Referência:\n",
    "[(Calculate TF-IDF in NLP (Simple Example)](https://youtu.be/vZAXpvHhQow)\n",
    "\n",
    "[scikit-exemplos](https://dadosaocubo.com/nlp-com-scikit-learn/)\n",
    "\n",
    "[Turing](https://github.com/turing-usp/BoW-e-TFIDF/blob/master/BoW_e_TFIDF.ipynb)\n",
    "\n",
    "https://www.computersciencemaster.com.br/como-implementar-o-tf-idf-em-python/\n",
    "\n",
    "https://medium.com/analytics-vidhya/demonstrating-calculation-of-tf-idf-from-sklearn-4f9526e7e78b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15e718a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_occurrences(string_list):\n",
    "    string_count = {}\n",
    "    for item in string_list:\n",
    "        if item not in string_count:\n",
    "            count = string_list.count(item)\n",
    "            string_count[item] = count\n",
    "    return string_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "599812d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocorrencias_termo= list(map(map_occurrences, corpus_sem_stopwors_tokenizada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0443a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tartaruga': 1,\n",
       "  'lenta': 1,\n",
       "  'pula': 1,\n",
       "  'sobr': 1,\n",
       "  'raposa': 1,\n",
       "  'preguiçosa': 1,\n",
       "  'rapasa': 1,\n",
       "  'baralho': 1},\n",
       " {'rápida': 1,\n",
       "  'raposa': 2,\n",
       "  'marrom': 1,\n",
       "  'pula': 1,\n",
       "  'sobr': 1,\n",
       "  'cão': 1,\n",
       "  'preguiçoso': 1,\n",
       "  'marota': 1}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocorrencias_termo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57450876",
   "metadata": {},
   "outputs": [],
   "source": [
    "termos_documento= list(map(len, corpus_sem_stopwors_tokenizada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a92f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termos_documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29fa6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}\n",
    "def calculaFT(ocorrencias_termo, termos_documento):\n",
    "    for i in range(len(ocorrencias_termo)):\n",
    "        result[i]= dict(map(lambda kv: (kv, ocorrencias_termo[i][kv]/termos_documento[i]), ocorrencias_termo[i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50e0a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencia_termo= calculaFT(ocorrencias_termo, termos_documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38f006ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'tartaruga': 0.125,\n",
       "  'lenta': 0.125,\n",
       "  'pula': 0.125,\n",
       "  'sobr': 0.125,\n",
       "  'raposa': 0.125,\n",
       "  'preguiçosa': 0.125,\n",
       "  'rapasa': 0.125,\n",
       "  'baralho': 0.125},\n",
       " 1: {'rápida': 0.1111111111111111,\n",
       "  'raposa': 0.2222222222222222,\n",
       "  'marrom': 0.1111111111111111,\n",
       "  'pula': 0.1111111111111111,\n",
       "  'sobr': 0.1111111111111111,\n",
       "  'cão': 0.1111111111111111,\n",
       "  'preguiçoso': 0.1111111111111111,\n",
       "  'marota': 0.1111111111111111}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia_termo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "236de824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "frequencia_termo_df = pd.concat({k: pd.Series(v) for k, v in frequencia_termo.items()}).reset_index()\n",
    "frequencia_termo_df.columns = ['corpus', 'palavra', 'TF ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e2d86ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>palavra</th>\n",
       "      <th>TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tartaruga</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>lenta</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>pula</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>sobr</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>raposa</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>preguiçosa</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>rapasa</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>baralho</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>rápida</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>raposa</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>marrom</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>pula</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>sobr</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>cão</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>preguiçoso</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>marota</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpus     palavra       TF \n",
       "0        0   tartaruga  0.125000\n",
       "1        0       lenta  0.125000\n",
       "2        0        pula  0.125000\n",
       "3        0        sobr  0.125000\n",
       "4        0      raposa  0.125000\n",
       "5        0  preguiçosa  0.125000\n",
       "6        0      rapasa  0.125000\n",
       "7        0     baralho  0.125000\n",
       "8        1      rápida  0.111111\n",
       "9        1      raposa  0.222222\n",
       "10       1      marrom  0.111111\n",
       "11       1        pula  0.111111\n",
       "12       1        sobr  0.111111\n",
       "13       1         cão  0.111111\n",
       "14       1  preguiçoso  0.111111\n",
       "15       1      marota  0.111111"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia_termo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64578d",
   "metadata": {},
   "source": [
    "# 5.2) Document Frequency (DF)\n",
    "𝐷𝐹 = 𝑞𝑡𝑑 𝑑𝑒 𝑜𝑐𝑜𝑟𝑟ê𝑛𝑐𝑖𝑎 𝑑𝑜 𝑡𝑒𝑟𝑚𝑜 𝑒𝑚 𝑢𝑚 𝑐𝑜𝑛𝑗𝑢𝑛𝑡𝑜 𝑑𝑒 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑜𝑠    \n",
    "https://kavita-ganesan.com/what-is-document-frequency/#.YagKOCVv894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba0c8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras= []\n",
    "for i in range (len(ocorrencias_termo)):\n",
    "    for item in list(ocorrencias_termo[i].keys()):\n",
    "        if item not in todas_palavras:\n",
    "            todas_palavras.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e23f8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_palavras_d1 = dict.fromkeys(todas_palavras, 0)\n",
    "for word in corpus_sem_stopwors_tokenizada[0]:\n",
    "        numero_palavras_d1[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f97e561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_palavras_d2 = dict.fromkeys(todas_palavras, 0)\n",
    "for word in corpus_sem_stopwors_tokenizada[1]:\n",
    "    numero_palavras_d2[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d8c866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = [numero_palavras_d1, numero_palavras_d2]\n",
    "document_frequency = dict.fromkeys(numero_palavras_d1.keys(), 0)\n",
    "for document in documentos:\n",
    "    for word, val in document.items():\n",
    "        if val > 0:\n",
    "            document_frequency[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99dd77b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tartaruga': 1,\n",
       " 'lenta': 1,\n",
       " 'pula': 2,\n",
       " 'sobr': 2,\n",
       " 'raposa': 2,\n",
       " 'preguiçosa': 1,\n",
       " 'rapasa': 1,\n",
       " 'baralho': 1,\n",
       " 'rápida': 1,\n",
       " 'marrom': 1,\n",
       " 'cão': 1,\n",
       " 'preguiçoso': 1,\n",
       " 'marota': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab28d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(document_frequency, orient='index', columns=['DF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aaa117",
   "metadata": {},
   "source": [
    "# 5.3) Inverse Document Frequency (IDF)\n",
    "𝐼𝐷𝐹 = 𝑙𝑜𝑔(𝑞𝑡𝑑 𝑑𝑒 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑜𝑠 / (𝐷𝐹 + 1))\n",
    "\n",
    "Referências:           \n",
    "[stackoverflow](https://stackoverflow.com/questions/48431173/is-there-a-way-to-get-only-the-idf-values-of-words-using-scikit-or-any-other-pyt)     \n",
    "[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ebf4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idf= {k: math.log10((len(corpus)/v)) for k, v in document_frequency.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b848e1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tartaruga': 0.3010299956639812,\n",
       " 'lenta': 0.3010299956639812,\n",
       " 'pula': 0.0,\n",
       " 'sobr': 0.0,\n",
       " 'raposa': 0.0,\n",
       " 'preguiçosa': 0.3010299956639812,\n",
       " 'rapasa': 0.3010299956639812,\n",
       " 'baralho': 0.3010299956639812,\n",
       " 'rápida': 0.3010299956639812,\n",
       " 'marrom': 0.3010299956639812,\n",
       " 'cão': 0.3010299956639812,\n",
       " 'preguiçoso': 0.3010299956639812,\n",
       " 'marota': 0.3010299956639812}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9740bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequency_mais_um = {k: v+1 for k, v in document_frequency.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9d6e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_ = {k: math.log10((len(corpus)/v)) for k, v in document_frequency_mais_um.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "987c9410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tartaruga': 0.0,\n",
       " 'lenta': 0.0,\n",
       " 'pula': -0.17609125905568127,\n",
       " 'sobr': -0.17609125905568127,\n",
       " 'raposa': -0.17609125905568127,\n",
       " 'preguiçosa': 0.0,\n",
       " 'rapasa': 0.0,\n",
       " 'baralho': 0.0,\n",
       " 'rápida': 0.0,\n",
       " 'marrom': 0.0,\n",
       " 'cão': 0.0,\n",
       " 'preguiçoso': 0.0,\n",
       " 'marota': 0.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16323271",
   "metadata": {},
   "source": [
    "# 5.4) TF-IDF\n",
    "𝑇𝐹 − 𝐼𝐷𝐹 = 𝐼𝐷𝐹 * 𝑇𝐹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98026033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'tartaruga': 0.125,\n",
       "  'lenta': 0.125,\n",
       "  'pula': 0.125,\n",
       "  'sobr': 0.125,\n",
       "  'raposa': 0.125,\n",
       "  'preguiçosa': 0.125,\n",
       "  'rapasa': 0.125,\n",
       "  'baralho': 0.125},\n",
       " 1: {'rápida': 0.1111111111111111,\n",
       "  'raposa': 0.2222222222222222,\n",
       "  'marrom': 0.1111111111111111,\n",
       "  'pula': 0.1111111111111111,\n",
       "  'sobr': 0.1111111111111111,\n",
       "  'cão': 0.1111111111111111,\n",
       "  'preguiçoso': 0.1111111111111111,\n",
       "  'marota': 0.1111111111111111}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia_termo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce651373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tartaruga': 0.3010299956639812,\n",
       " 'lenta': 0.3010299956639812,\n",
       " 'pula': 0.0,\n",
       " 'sobr': 0.0,\n",
       " 'raposa': 0.0,\n",
       " 'preguiçosa': 0.3010299956639812,\n",
       " 'rapasa': 0.3010299956639812,\n",
       " 'baralho': 0.3010299956639812,\n",
       " 'rápida': 0.3010299956639812,\n",
       " 'marrom': 0.3010299956639812,\n",
       " 'cão': 0.3010299956639812,\n",
       " 'preguiçoso': 0.3010299956639812,\n",
       " 'marota': 0.3010299956639812}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_idf(palavra):\n",
    "    for chave, valor in idf.items():\n",
    "         if chave == palavra:\n",
    "             return valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "retorna_idf('rápida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e038ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ocorrencias_termo)):\n",
    "    for k,v in frequencia_termo[0].items():\n",
    "        print(i)\n",
    "        print(k, v)\n",
    "        print(retorna_idf(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2824bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(TF_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = dict(map(lambda kv: (kv, frequencia_termo[0][kv]/idf), frequencia_termo[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "retorna_idf('lenta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd5239",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencia_termo_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in frequencia_termo.items():\n",
    "    print(k, \"-\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e97cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf= {k: math.log10((len(corpus)/v)) for k, v in document_frequency.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b00bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae6b4fc",
   "metadata": {},
   "source": [
    "# 5.5) Lista de strings com proximidade até 2 dos 5 termos de maior TF-IDF. Essas strings devem ser acompanhadas de seu valor de TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212083dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28c1cf29",
   "metadata": {},
   "source": [
    "# 6) Gerar um arquivo csv que possui todas as palavras de todos os documentos na primeira coluna, em que cada linha é um token. Para cada token, informe nas colunas vizinhas as informações determinadas no objetivo 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e9403",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "173cd86b",
   "metadata": {},
   "source": [
    "# 7) Gerar nuvem de palavras para análise visual tal como exemplo abaixo. Cada ponto central será um dos 5 termos de maior TF-IDF. As conexões são as palavras próximas obtidas em 5.4. O tamanho do círculo da palavra é baseado no TF dela. O maior círculo que conecta o termo central será normalizado para palavras de maior TF do conjunto (desafio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37611eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0db9b905",
   "metadata": {},
   "source": [
    "# Tópicos de Auxílio\n",
    "Se realizada a lematização manual, os resultados seguintes são duplicados e a comparação será realizada analisando a nuvem de palavras de cada implementação.\n",
    "Informações sobre as métricas utilizadas\n",
    "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-worlddataset-\n",
    "796d339a4089\n",
    "Atividade desafio de determinação da nuvem de palavras\n",
    "https://www.kaggle.com/arthurtok/ghastly-network-and-d3-js-force-directed-graphs\n",
    "\n",
    "http://andrewtrick.com/stormlight_network.html\n",
    "\n",
    "https://github.com/corymaklin/tfidf/blob/master/tfidf.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "my_data = [\"hello how are you\", \"hello who are you\", \"i am not you\"]\n",
    "\n",
    "tf = TfidfVectorizer(use_idf=True)\n",
    "tf.fit_transform(my_data)\n",
    "\n",
    "idf = tf.idf_ \n",
    "\n",
    "#[BONUS] if you want to get the idf value for a particular word:\n",
    "\n",
    "# If you want to get the idf value for a particular word, here \"hello\"    \n",
    "tf.idf_[tf.vocabulary_[\"hello\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21685e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf341e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
