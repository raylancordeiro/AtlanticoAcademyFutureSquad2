{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122c10ed",
   "metadata": {},
   "source": [
    "# Atlântico Academy Future\n",
    "### VISÃO GERAL\n",
    "Uma empresa contratante deseja estabelecer termos de maior relevância em um documento específico. Neste caso, considere o histórico de exames, consultas e procedimentos realizados por um paciente. Um sistema deve ser desenvolvido para que o médico possa ter uma visão geral do histórico do paciente sem a necessidade de analisar documento por documento. Com\n",
    "base nesta importância, vamos desenvolver uma etapa deste sistema. Tokenizar um texto,realizar remoção de stopwords, aplicar o processo de lematização e fazer uma análise quantitativa deste. Neste caso, vamos comparar duas estratégias , se possível. A primeira utilizando a lib stanza e a segunda uma análise com base em acesso a um dicionário."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab2a4d",
   "metadata": {},
   "source": [
    "# 1) Carregar o conjunto de documentos em PDF e armazená-los em alguma estrutura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911d1ec7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_pdf_file(file):\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    page_content_all= ''\n",
    "    for i in range(number_of_pages):\n",
    "        page_content = read_pdf.getPage(i).extractText()\n",
    "        page_content_all += page_content\n",
    "    return page_content_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8706664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pfds(patch_files):\n",
    "    os.chdir(patch_files)\n",
    "    file_base_all = []\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_base_all.append(read_pdf_file(os.path.join(patch_files, file)))\n",
    "    return file_base_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98de2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_and_remove_breaks(base):\n",
    "    list_parsed = []\n",
    "    for i in range(len(base)):\n",
    "        list_parsed.append(re.sub('\\n', '', base[i]))\n",
    "    return list_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad73f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = join_and_remove_breaks(load_pfds(r'C:\\Users\\r211315\\Documents\\ia_express\\arquivos'))\n",
    "corpus = join_and_remove_breaks(load_pfds(r'/home/vagnersv/my_tensorflow/arquivos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7895825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "693a1819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uma rápida raposa marrom pula sobre o cão preguiçoso. Que a raposa! ',\n",
       " 'Uma rápida raposa marrom pula sobre a raposa preguiçoso. QUE RAPASA DO BARALHO! ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe1ee7",
   "metadata": {},
   "source": [
    "# 2) Realizar o pré-processamento destes ( tokenização e remoção de stop words, deixar todos os caracteres minúsculos...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a1000",
   "metadata": {},
   "source": [
    "Remoção de Palavras Vazias(e.g., artigos, preposições, etc.), que possuem alta frequência em todos os documentos, podem ser removidas da contagem para melhorar a distinção entre documentos\n",
    "\n",
    "[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77ccf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i]= corpus[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a807afe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uma rápida raposa marrom pula sobre o cão preguiçoso. que a raposa! ',\n",
       " 'uma rápida raposa marrom pula sobre a raposa preguiçoso. que rapasa do baralho! ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e84ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vagnersv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "def removestopwords(texto):\n",
    "    frases = []\n",
    "    for palavras in texto:\n",
    "        semstop = [p for p in palavras.split() if p not in stopwords]\n",
    "        frases.append(semstop)\n",
    "    return frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81eaa492",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sem_stopwords = removestopwords(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47bd9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(corpus, seperator=' '):\n",
    "    return  seperator.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afe83b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sem_stopwords= list(map(convert_list_to_string, corpus_sem_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd9f285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rápida raposa marrom pula sobre cão preguiçoso. raposa!',\n",
       " 'rápida raposa marrom pula sobre raposa preguiçoso. rapasa baralho!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sem_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd1e58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "#######\n",
    "# https://stackoverflow.com/questions/26126442/combining-text-stemming-and-removal-of-punctuation-in-nltk-and-scikit-learn\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(texto):\n",
    "    tokens = nltk.word_tokenize(texto)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "542f9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sem_stopwors_tokenizada= list(map(tokenize, corpus_sem_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e7d0c",
   "metadata": {},
   "source": [
    "# 3) Lematização com a Lib stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b08d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stanza_portugues():\n",
    "    \"\"\"\n",
    "    Faz o download do stanza em portugues\n",
    "    \"\"\"\n",
    "    stanza.download(lang='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "872d6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só é necessário executar o download uma vez\n",
    "#download_stanza_portugues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24cb363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_and_lemmatizer(text):\n",
    "    \"\"\"\n",
    "        Performs tokenization and lemmatization on input text\n",
    "\n",
    "    Args:\n",
    "        text: A string with the content of the text\n",
    "\n",
    "    Returns:\n",
    "        A stanza Document with the tokens and lemmas\n",
    "\n",
    "    \"\"\"\n",
    "    nlp = stanza.Pipeline('pt', processors='tokenize,mwt,pos,lemma')\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "123613ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nlp_doc(doc):\n",
    "    \"\"\"\n",
    "    Imprime os tokens (somente para debug)\n",
    "    \"\"\"\n",
    "    sentence_id = 0\n",
    "    for sentence in doc.sentences:\n",
    "        sentence_id += 1\n",
    "        print('\\nSentença {}:'.format(sentence_id))\n",
    "        for word in sentence.words:\n",
    "            print('palavra = {}, lema = {}, id = {}'.format(word.text, word.lemma, word.id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31956673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory_path = r'C:\\Users\\r211315\\Documents\\ia_express\\arquivos'\n",
    "directory_path = r'/home/vagnersv/my_tensorflow/arquivos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38a2db57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 22:28:40 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| pos       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "=======================\n",
      "\n",
      "2021-12-01 22:28:41 INFO: Use device: cpu\n",
      "2021-12-01 22:28:41 INFO: Loading: tokenize\n",
      "2021-12-01 22:28:41 INFO: Loading: mwt\n",
      "2021-12-01 22:28:41 INFO: Loading: pos\n",
      "2021-12-01 22:28:43 INFO: Loading: lemma\n",
      "2021-12-01 22:28:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "txt3_processed = tokenizer_and_lemmatizer(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a694de2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentença 1:\n",
      "palavra = uma, lema = um, id = 1\n",
      "palavra = rápida, lema = rápido, id = 2\n",
      "palavra = raposa, lema = raposa, id = 3\n",
      "palavra = marrom, lema = marrom, id = 4\n",
      "palavra = pula, lema = pular, id = 5\n",
      "palavra = sobre, lema = sobre, id = 6\n",
      "palavra = o, lema = o, id = 7\n",
      "palavra = cão, lema = cão, id = 8\n",
      "palavra = preguiçoso, lema = preguiçoso, id = 9\n",
      "palavra = ., lema = ., id = 10\n",
      "\n",
      "Sentença 2:\n",
      "palavra = que, lema = que, id = 1\n",
      "palavra = a, lema = o, id = 2\n",
      "palavra = raposa, lema = raposa, id = 3\n",
      "palavra = !, lema = !, id = 4\n"
     ]
    }
   ],
   "source": [
    "show_nlp_doc(txt3_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08846145",
   "metadata": {},
   "source": [
    "# 4) Lematização manual com inspiração no trabalho descrito no [link](https://github.com/rikarudo/LemPORT) (Atividade desafio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6cafbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a6ff37f",
   "metadata": {},
   "source": [
    "# 5) Implementar API para determinar as seguintes informações do resultados obtidos em 3 e/ou 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4a411",
   "metadata": {},
   "source": [
    "# 5.1) Term Frequency (TF):\n",
    "𝑇𝐹 = 𝑞𝑡𝑑 𝑑𝑒 𝑜𝑐𝑜𝑟𝑟ê𝑛𝑐𝑖𝑎 𝑑𝑜 𝑡𝑒𝑟𝑚𝑜 𝑒𝑚 𝑢𝑚 𝑡𝑒𝑥𝑡𝑜 / 𝑞𝑢𝑎𝑛𝑡𝑖𝑑𝑎𝑑𝑒 𝑡𝑜𝑡𝑎𝑙 𝑑𝑒 𝑝𝑎𝑙𝑎𝑣𝑟𝑎𝑠 𝑑𝑜 𝑡𝑒𝑥𝑡𝑜\n",
    "\n",
    "Referência:\n",
    "[(Calculate TF-IDF in NLP (Simple Example)](https://youtu.be/vZAXpvHhQow)\n",
    "\n",
    "[scikit-exemplos](https://dadosaocubo.com/nlp-com-scikit-learn/)\n",
    "\n",
    "[Turing](https://github.com/turing-usp/BoW-e-TFIDF/blob/master/BoW_e_TFIDF.ipynb)\n",
    "\n",
    "https://www.computersciencemaster.com.br/como-implementar-o-tf-idf-em-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15e718a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_occurrences(string_list):\n",
    "    string_count = {}\n",
    "    for item in string_list:\n",
    "        if item not in string_count:\n",
    "            count = string_list.count(item)\n",
    "            string_count[item] = count\n",
    "    return string_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "599812d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocorrencias_termo= list(map(map_occurrences, corpus_sem_stopwors_tokenizada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0443a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rápida': 1,\n",
       "  'raposa': 2,\n",
       "  'marrom': 1,\n",
       "  'pula': 1,\n",
       "  'sobr': 1,\n",
       "  'cão': 1,\n",
       "  'preguiçoso': 1},\n",
       " {'rápida': 1,\n",
       "  'raposa': 2,\n",
       "  'marrom': 1,\n",
       "  'pula': 1,\n",
       "  'sobr': 1,\n",
       "  'preguiçoso': 1,\n",
       "  'rapasa': 1,\n",
       "  'baralho': 1}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocorrencias_termo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57450876",
   "metadata": {},
   "outputs": [],
   "source": [
    "termos_documento= list(map(len, corpus_sem_stopwors_tokenizada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a92f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termos_documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29fa6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}\n",
    "def calculaFT(ocorrencias_termo, termos_documento):\n",
    "    for i in range(len(ocorrencias_termo)):\n",
    "        result[i]= dict(map(lambda kv: (kv, ocorrencias_termo[i][kv]/termos_documento[i]), ocorrencias_termo[i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50e0a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencia_termo= calculaFT(ocorrencias_termo, termos_documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "236de824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "frequencia_termo_df = pd.concat({k: pd.Series(v) for k, v in frequencia_termo.items()}).reset_index()\n",
    "frequencia_termo_df.columns = ['corpus', 'palavra', 'TF ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e2d86ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>palavra</th>\n",
       "      <th>TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>rápida</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>raposa</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>marrom</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>pula</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>sobr</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>cão</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>preguiçoso</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>rápida</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>raposa</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>marrom</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>pula</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>sobr</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>preguiçoso</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>rapasa</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>baralho</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpus     palavra       TF \n",
       "0        0      rápida  0.125000\n",
       "1        0      raposa  0.250000\n",
       "2        0      marrom  0.125000\n",
       "3        0        pula  0.125000\n",
       "4        0        sobr  0.125000\n",
       "5        0         cão  0.125000\n",
       "6        0  preguiçoso  0.125000\n",
       "7        1      rápida  0.111111\n",
       "8        1      raposa  0.222222\n",
       "9        1      marrom  0.111111\n",
       "10       1        pula  0.111111\n",
       "11       1        sobr  0.111111\n",
       "12       1  preguiçoso  0.111111\n",
       "13       1      rapasa  0.111111\n",
       "14       1     baralho  0.111111"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia_termo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1a3dd",
   "metadata": {},
   "source": [
    "###  Teste com o sciki-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2565c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sem_stopwors_tokenizada_list = []\n",
    "for i in range (len(corpus)):\n",
    "    corpus_sem_stopwors_tokenizada_list.append(convert_list_to_string(corpus_sem_stopwors_tokenizada[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9ef12f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário\n",
      "['baralho', 'cão', 'marrom', 'preguiçoso', 'pula', 'rapasa', 'raposa', 'rápida', 'sobr']\n",
      "\n",
      "Matrix\n",
      "[[0 1 1 1 1 0 2 1 1]\n",
      " [1 0 1 1 1 1 2 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. [deprecation.py:87]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vetores = vectorizer.fit_transform(corpus_sem_stopwors_tokenizada_list)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "print('Vocabulário')\n",
    "print(vocab)\n",
    "print()\n",
    "print('Matrix')\n",
    "print(vetores.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eda021ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário\n",
      "['baralho' 'cão' 'marrom' 'preguiçoso' 'pula' 'rapasa' 'raposa' 'rápida'\n",
      " 'sobr']\n",
      "\n",
      "Matrix\n",
      "[[0.   0.32 0.32 0.32 0.32 0.   0.63 0.32 0.32]\n",
      " [0.3  0.   0.3  0.3  0.3  0.3  0.6  0.3  0.3 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = Pipeline([('count', CountVectorizer()),('tfid', TfidfTransformer(use_idf=False))])\n",
    "\n",
    "#vectorizer = Pipeline([('count', CountVectorizer()),('tfid', TfidfTransformer())])\n",
    "\n",
    "vetores = vectorizer.fit_transform(corpus_sem_stopwors_tokenizada_list)\n",
    "vocab = vectorizer['count'].get_feature_names_out()\n",
    "\n",
    "print('Vocabulário')\n",
    "print(vocab)\n",
    "print()\n",
    "print('Matrix')\n",
    "print(np.round(vetores.toarray(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea90da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário\n",
      "['baralho', 'cão', 'marrom', 'preguiçoso', 'pula', 'rapasa', 'raposa', 'rápida', 'sobr']\n",
      "\n",
      "Matrix\n",
      "[[0.   0.42 0.3  0.3  0.3  0.   0.6  0.3  0.3 ]\n",
      " [0.39 0.   0.28 0.28 0.28 0.39 0.56 0.28 0.28]]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = Pipeline([('count', CountVectorizer(analyzer='word')),\n",
    "                 ('tfid', TfidfTransformer())])\n",
    "\n",
    "vetores = vectorizer.fit_transform(corpus_sem_stopwors_tokenizada_list)\n",
    "vocab = vectorizer['count'].get_feature_names()\n",
    "\n",
    "print('Vocabulário')\n",
    "print(vocab)\n",
    "print()\n",
    "print('Matrix')\n",
    "print(np.round(vetores.toarray(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "308324cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['baralho', 'cão', 'marrom', 'preguiçoso', 'pula', 'rapasa',\n",
       "       'raposa', 'rápida', 'sobr'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus_sem_stopwors_tokenizada_list)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a73efd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 0 2 1 1]\n",
      " [1 0 1 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd248852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rápida': 7,\n",
       " 'raposa': 6,\n",
       " 'marrom': 2,\n",
       " 'pula': 4,\n",
       " 'sobr': 8,\n",
       " 'cão': 1,\n",
       " 'preguiçoso': 3,\n",
       " 'rapasa': 5,\n",
       " 'baralho': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "847148da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baralho', 'cão', 'marrom', 'preguiçoso', 'pula', 'rapasa', 'raposa', 'rápida', 'sobr']\n",
      "[[0 1 1 1 1 0 2 1 1]\n",
      " [1 0 1 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(corpus_sem_stopwors_tokenizada_list)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c53cea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 2 2 1 4 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(cv_fit.toarray().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86fa5170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baralho',\n",
       " 'cão',\n",
       " 'marrom',\n",
       " 'preguiçoso',\n",
       " 'pula',\n",
       " 'rapasa',\n",
       " 'raposa',\n",
       " 'rápida',\n",
       " 'sobr']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = cv.get_feature_names()\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24e5fa2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 2, 1, 4, 2, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_list = np.asarray(X.sum(axis=0)).ravel()\n",
    "count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73b0463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'baralho': 1, 'cão': 1, 'marrom': 2, 'preguiçoso': 2, 'pula': 2, 'rapasa': 1, 'raposa': 4, 'rápida': 2, 'sobr': 2}\n"
     ]
    }
   ],
   "source": [
    "print(dict(zip(word_list, count_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70ad816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    baralho       cão    marrom  preguiçoso      pula    rapasa    raposa  \\\n",
      "0  0.000000  0.447999  0.318755    0.318755  0.318755  0.000000  0.539699   \n",
      "1  0.408845  0.000000  0.290897    0.290897  0.290897  0.408845  0.492531   \n",
      "\n",
      "     rápida      sobr  \n",
      "0  0.318755  0.318755  \n",
      "1  0.290897  0.290897  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.csr import csr_matrix #need this if you want to save tfidf_matrix\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "#tf = TfidfVectorizer(analyzer='word', stop_words=stopwords, min_df = 0, sublinear_tf=True, use_idf=True)\n",
    "tf = TfidfVectorizer(analyzer='word', stop_words=stopwords, min_df = 0, sublinear_tf=True, use_idf=True)\n",
    "#tf = TfidfVectorizer(analyzer='word', stop_words=stopwords, min_df = 0)\n",
    "tfidf_matrix =  tf.fit_transform(corpus_sem_stopwors_tokenizada_list)\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns = tf.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecba0621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baralho' 'cão' 'marrom' 'preguiçoso' 'pula' 'rapasa' 'raposa' 'rápida'\n",
      " 'sobr']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tf.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49075717",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/demonstrating-calculation-of-tf-idf-from-sklearn-4f9526e7e78b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64578d",
   "metadata": {},
   "source": [
    "# 5.2) Document Frequency (DF)\n",
    "𝐷𝐹 = 𝑞𝑡𝑑 𝑑𝑒 𝑜𝑐𝑜𝑟𝑟ê𝑛𝑐𝑖𝑎 𝑑𝑜 𝑡𝑒𝑟𝑚𝑜 𝑒𝑚 𝑢𝑚 𝑐𝑜𝑛𝑗𝑢𝑛𝑡𝑜 𝑑𝑒 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑜𝑠    \n",
    "https://kavita-ganesan.com/what-is-document-frequency/#.YagKOCVv894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba0c8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras= []\n",
    "for i in range (len(ocorrencias_termo)):\n",
    "    for item in list(ocorrencias_termo[i].keys()):\n",
    "        if item not in todas_palavras:\n",
    "            todas_palavras.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e23f8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_palavras_d1 = dict.fromkeys(todas_palavras, 0)\n",
    "for word in corpus_sem_stopwors_tokenizada[0]:\n",
    "        numero_palavras_d1[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5216ffb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rápida': 1,\n",
       " 'raposa': 2,\n",
       " 'marrom': 1,\n",
       " 'pula': 1,\n",
       " 'sobr': 1,\n",
       " 'cão': 1,\n",
       " 'preguiçoso': 1,\n",
       " 'rapasa': 0,\n",
       " 'baralho': 0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numero_palavras_d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f97e561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_palavras_d2 = dict.fromkeys(todas_palavras, 0)\n",
    "for word in corpus_sem_stopwors_tokenizada[1]:\n",
    "    numero_palavras_d2[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d8c866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = [numero_palavras_d1, numero_palavras_d2]\n",
    "document_frequency = dict.fromkeys(numero_palavras_d1.keys(), 0)\n",
    "for document in documentos:\n",
    "    for word, val in document.items():\n",
    "        if val > 0:\n",
    "            document_frequency[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49b548f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rápida': 1,\n",
       "  'raposa': 2,\n",
       "  'marrom': 1,\n",
       "  'pula': 1,\n",
       "  'sobr': 1,\n",
       "  'cão': 1,\n",
       "  'preguiçoso': 1,\n",
       "  'rapasa': 0,\n",
       "  'baralho': 0},\n",
       " {'rápida': 1,\n",
       "  'raposa': 2,\n",
       "  'marrom': 1,\n",
       "  'pula': 1,\n",
       "  'sobr': 1,\n",
       "  'cão': 0,\n",
       "  'preguiçoso': 1,\n",
       "  'rapasa': 1,\n",
       "  'baralho': 1}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99dd77b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rápida': 2,\n",
       " 'raposa': 2,\n",
       " 'marrom': 2,\n",
       " 'pula': 2,\n",
       " 'sobr': 2,\n",
       " 'cão': 1,\n",
       " 'preguiçoso': 2,\n",
       " 'rapasa': 1,\n",
       " 'baralho': 1}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eab28d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rápida</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raposa</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marrom</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pula</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sobr</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cão</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preguiçoso</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rapasa</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baralho</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            DF\n",
       "rápida       2\n",
       "raposa       2\n",
       "marrom       2\n",
       "pula         2\n",
       "sobr         2\n",
       "cão          1\n",
       "preguiçoso   2\n",
       "rapasa       1\n",
       "baralho      1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(document_frequency, orient='index', columns=['DF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aaa117",
   "metadata": {},
   "source": [
    "# 5.3) Inverse Document Frequency (IDF)\n",
    "𝐼𝐷𝐹 = 𝑙𝑜𝑔(𝑞𝑡𝑑 𝑑𝑒 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑜𝑠 / (𝐷𝐹 + 1))\n",
    "\n",
    "Referências:           \n",
    "[stackoverflow](https://stackoverflow.com/questions/48431173/is-there-a-way-to-get-only-the-idf-values-of-words-using-scikit-or-any-other-pyt)     \n",
    "[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c84955b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer(use_idf=True)\n",
    "tf.fit_transform(corpus)\n",
    "\n",
    "idf = tf.idf_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d295fccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40546511, 1.40546511, 1.40546511, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.40546511, 1.        , 1.        ,\n",
       "       1.        , 1.        ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b4792e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4054651081081644"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf[tf.vocabulary_[\"cão\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54f1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbf9cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sem_stopwors_tokenizada= list(map(convert_list_to_string, \n",
    "                                         corpus_sem_stopwors_tokenizada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e530e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rápida raposa marrom pula sobr cão preguiçoso raposa',\n",
       " 'rápida raposa marrom pula sobr raposa preguiçoso rapasa baralho']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sem_stopwors_tokenizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "629978f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer(use_idf=True)\n",
    "tf.fit_transform(corpus_sem_stopwors_tokenizada)\n",
    "\n",
    "idf = tf.idf_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "038dc91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n"
     ]
    }
   ],
   "source": [
    "print(idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8958f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40546511, 1.40546511, 1.        , 1.        , 1.        ,\n",
       "       1.40546511, 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7a36a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4054651081081644"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf[tf.vocabulary_[\"cão\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16323271",
   "metadata": {},
   "source": [
    "# 5.4) TF-IDF\n",
    "𝑇𝐹 − 𝐼𝐷𝐹 = 𝐼𝐷𝐹 * 𝑇𝐹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c020135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário\n",
      "['baralho' 'cão' 'marrom' 'preguiçoso' 'pula' 'rapasa' 'raposa' 'rápida'\n",
      " 'sobre']\n",
      "\n",
      "Matrix\n",
      "[[0.   0.32 0.32 0.32 0.32 0.   0.63 0.32 0.32]\n",
      " [0.3  0.   0.3  0.3  0.3  0.3  0.6  0.3  0.3 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vagnersv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "#######\n",
    "# https://stackoverflow.com/questions/26126442/combining-text-stemming-and-removal-of-punctuation-in-nltk-and-scikit-learn\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(texto):\n",
    "    tokens = nltk.word_tokenize(texto)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer = Pipeline([('count', CountVectorizer(analyzer='word', stop_words=stopwords)),\n",
    "                 ('tfid', TfidfTransformer(use_idf=False))])\n",
    "\n",
    "vetores = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer['count'].get_feature_names_out()\n",
    "\n",
    "print('Vocabulário')\n",
    "print(vocab)\n",
    "print()\n",
    "print('Matrix')\n",
    "print(np.round(vetores.toarray(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6b4fc",
   "metadata": {},
   "source": [
    "# 5.5) Lista de strings com proximidade até 2 dos 5 termos de maior TF-IDF. Essas strings devem ser acompanhadas de seu valor de TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212083dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28c1cf29",
   "metadata": {},
   "source": [
    "# 6) Gerar um arquivo csv que possui todas as palavras de todos os documentos na primeira coluna, em que cada linha é um token. Para cada token, informe nas colunas vizinhas as informações determinadas no objetivo 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e9403",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "173cd86b",
   "metadata": {},
   "source": [
    "# 7) Gerar nuvem de palavras para análise visual tal como exemplo abaixo. Cada ponto central será um dos 5 termos de maior TF-IDF. As conexões são as palavras próximas obtidas em 5.4. O tamanho do círculo da palavra é baseado no TF dela. O maior círculo que conecta o termo central será normalizado para palavras de maior TF do conjunto (desafio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37611eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0db9b905",
   "metadata": {},
   "source": [
    "# Tópicos de Auxílio\n",
    "Se realizada a lematização manual, os resultados seguintes são duplicados e a comparação será realizada analisando a nuvem de palavras de cada implementação.\n",
    "Informações sobre as métricas utilizadas\n",
    "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-worlddataset-\n",
    "796d339a4089\n",
    "Atividade desafio de determinação da nuvem de palavras\n",
    "https://www.kaggle.com/arthurtok/ghastly-network-and-d3-js-force-directed-graphs\n",
    "\n",
    "http://andrewtrick.com/stormlight_network.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ead7ed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2876820724517808"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "my_data = [\"hello how are you\", \"hello who are you\", \"i am not you\"]\n",
    "\n",
    "tf = TfidfVectorizer(use_idf=True)\n",
    "tf.fit_transform(my_data)\n",
    "\n",
    "idf = tf.idf_ \n",
    "\n",
    "#[BONUS] if you want to get the idf value for a particular word:\n",
    "\n",
    "# If you want to get the idf value for a particular word, here \"hello\"    \n",
    "tf.idf_[tf.vocabulary_[\"hello\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21685e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "daf341e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are you'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
